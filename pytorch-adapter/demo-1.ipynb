{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa891d87-f257-4f5b-ad46-f01a077a7d9c",
   "metadata": {},
   "source": [
    "This notebook demonstrates the API of the `cvat_sdk.pytorch` module in CVAT SDK.\n",
    "It is part of the supplementary materials for the article \"PyTorch adapter in the CVAT SDK\".\n",
    "\n",
    "To run it, first fill in your CVAT access credentials in the cell below.\n",
    "You also need to run the `upload-flowers.py` script and fill in the ID of the training task that it has printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d860fb0-93e1-4d22-a1e0-0af6c1aaa3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CVAT_HOST = 'app.cvat.ai' # the hostname of your CVAT instance\n",
    "CVAT_USER = '...' # your username\n",
    "CVAT_PASS = '...' # your password\n",
    "\n",
    "TRAIN_TASK_ID = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011dc11-d9ce-4c32-a754-5a4103dc9790",
   "metadata": {},
   "source": [
    "First, we need to create a CVAT API client.\n",
    "This will log into the CVAT server and verify the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383943d-4805-4e01-aa61-f243f4c73822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging, os\n",
    "from cvat_sdk import *\n",
    "\n",
    "# configure logging to see what the SDK is doing behind the scenes\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "client = make_client(CVAT_HOST, credentials=(CVAT_USER, CVAT_PASS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b993ee-05f3-46dc-8f0f-46a9ea045fef",
   "metadata": {},
   "source": [
    "Now we can create a `TaskVisionDataset` instance representing our training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198b26d-1325-41c2-80c7-679ce0880c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cvat_sdk.pytorch import *\n",
    "\n",
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7977fce-5b5a-4def-bda8-d08aef6797ad",
   "metadata": {},
   "source": [
    "We can verify that `train_set` is an instance of the PyTorch `Dataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c9bf6-e32f-47c2-b98f-7aaad1a03831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "isinstance(train_set, torch.utils.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c925e-ffe1-4439-84ac-3f3a05cacbec",
   "metadata": {},
   "source": [
    "As such, we can query its length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b2448-e26b-4949-921e-123215e2744c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6da843-e33f-4648-a495-d88a87e8fe2a",
   "metadata": {},
   "source": [
    "And we can query individual samples. The first component of each sample is an image (an instance of the `PIL.Image` class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd659c4-6c1a-4350-9e9c-0f933df2c72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a0ea2-0b73-41f4-b3a0-0a3d5fe0e7b5",
   "metadata": {},
   "source": [
    "And the second component is a `Target` object containing the annotations associated with the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb1fb4-74a0-425c-967b-3e53e1808679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccdede-8598-4f56-b951-43b64134a197",
   "metadata": {},
   "source": [
    "To simplify the target component in a simple classification scenario,\n",
    "you can use the `ExtractSingleLabelIndex` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a76d53-532a-413f-ba78-8f311392f980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID, \n",
    "      target_transform=ExtractSingleLabelIndex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c8ae0-1286-42fd-b4e4-6ace1000644a",
   "metadata": {},
   "source": [
    "When this transform is applied,\n",
    "the target component becomes a zero-dimensional tensor containing the label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49c19a-ed1b-4ec6-9736-41972e98d864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(train_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbf965-8a4c-4016-a12c-0518a0405d2c",
   "metadata": {},
   "source": [
    "To use the samples as inputs to PyTorch models,\n",
    "you'll also need to transform the image component.\n",
    "torchvision already includes a variety of image transforms,\n",
    "which you can use via the `transform` argument.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d43ef-7267-49f2-a8d9-8548699e5768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID,\n",
    "    transform=transforms.ToTensor(),\n",
    "    target_transform=ExtractSingleLabelIndex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd9eae-3e2b-4c4c-8234-5a403f49de09",
   "metadata": {},
   "source": [
    "Now the image component is a PyTorch tensor too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c34051-0ba4-4c46-826f-22ada341d7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
