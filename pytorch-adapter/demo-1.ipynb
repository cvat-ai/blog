{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa891d87-f257-4f5b-ad46-f01a077a7d9c",
   "metadata": {},
   "source": [
    "This notebook demonstrates the API of the `cvat_sdk.pytorch` module in CVAT SDK.\n",
    "It is part of the supplementary materials for the article [\"CVAT SDK PyTorch adapter: using CVAT datasets in your ML pipeline\"](https://www.cvat.ai/post/cvat-sdk-pytorch-adapter).\n",
    "\n",
    "To run it, first fill in your CVAT access credentials in the cell below.\n",
    "You also need to run the `upload-flowers.py` script and fill in the ID of the training task that it has printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d860fb0-93e1-4d22-a1e0-0af6c1aaa3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CVAT_HOST = 'app.cvat.ai' # the hostname of your CVAT instance\n",
    "CVAT_USER = '...' # your username\n",
    "CVAT_PASS = '...' # your password\n",
    "\n",
    "TRAIN_TASK_ID = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011dc11-d9ce-4c32-a754-5a4103dc9790",
   "metadata": {},
   "source": [
    "First, we need to create a CVAT API client.\n",
    "This will log into the CVAT server and verify the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383943d-4805-4e01-aa61-f243f4c73822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging, os\n",
    "from cvat_sdk import *\n",
    "\n",
    "# configure logging to see what the SDK is doing behind the scenes\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "client = make_client(CVAT_HOST, credentials=(CVAT_USER, CVAT_PASS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b993ee-05f3-46dc-8f0f-46a9ea045fef",
   "metadata": {},
   "source": [
    "Now we can create a `TaskVisionDataset` instance representing our training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198b26d-1325-41c2-80c7-679ce0880c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cvat_sdk.pytorch import *\n",
    "\n",
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7977fce-5b5a-4def-bda8-d08aef6797ad",
   "metadata": {},
   "source": [
    "We can verify that `train_set` is an instance of the PyTorch `Dataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c9bf6-e32f-47c2-b98f-7aaad1a03831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "isinstance(train_set, torch.utils.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c925e-ffe1-4439-84ac-3f3a05cacbec",
   "metadata": {},
   "source": [
    "As such, we can query its length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b2448-e26b-4949-921e-123215e2744c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6da843-e33f-4648-a495-d88a87e8fe2a",
   "metadata": {},
   "source": [
    "And we can query individual samples. The first component of each sample is an image (an instance of the `PIL.Image` class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd659c4-6c1a-4350-9e9c-0f933df2c72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a0ea2-0b73-41f4-b3a0-0a3d5fe0e7b5",
   "metadata": {},
   "source": [
    "And the second component is a `Target` object containing the annotations associated with the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb1fb4-74a0-425c-967b-3e53e1808679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccdede-8598-4f56-b951-43b64134a197",
   "metadata": {},
   "source": [
    "To simplify the target component in a simple classification scenario,\n",
    "you can use the `ExtractSingleLabelIndex` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a76d53-532a-413f-ba78-8f311392f980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID, \n",
    "      target_transform=ExtractSingleLabelIndex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c8ae0-1286-42fd-b4e4-6ace1000644a",
   "metadata": {},
   "source": [
    "When this transform is applied,\n",
    "the target component becomes a zero-dimensional tensor containing the label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49c19a-ed1b-4ec6-9736-41972e98d864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(train_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbf965-8a4c-4016-a12c-0518a0405d2c",
   "metadata": {},
   "source": [
    "To use the samples as inputs to PyTorch models,\n",
    "you'll also need to transform the image component.\n",
    "torchvision already includes a variety of image transforms,\n",
    "which you can use via the `transform` argument.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d43ef-7267-49f2-a8d9-8548699e5768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set = TaskVisionDataset(client, TRAIN_TASK_ID,\n",
    "    transform=transforms.ToTensor(),\n",
    "    target_transform=ExtractSingleLabelIndex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd9eae-3e2b-4c4c-8234-5a403f49de09",
   "metadata": {},
   "source": [
    "Now the image component is a PyTorch tensor too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c34051-0ba4-4c46-826f-22ada341d7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
